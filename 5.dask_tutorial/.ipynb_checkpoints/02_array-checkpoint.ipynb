{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" align=\"right\" width=\"30%\" alt=\"Dask logo\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Arrays - Parallelized numpy\n",
    "Parallel n-dimensional arrays, larger than system memory, using blocked algorithms.\n",
    "\n",
    "* **Parallel**: Uses all the cores of your computer.\n",
    "* **Larger than memory**: Allows you to work on datasets that are larger than your available memory by breaking your array into many small chunks, operating on those chunks in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "* **Blocked algorithms**: Perform large computations by executing many smaller computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-array.svg\" width=\"40%\" align=\"right\">\n",
    "\n",
    "In other words, Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, breaking the large array into many small arrays. This allows us to compute on arrays larger than memory using all our cores. We coordinate these blocked algorithms using Dask graphs.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Array documentation](https://docs.dask.org/en/latest/array.html)\n",
    "* [Array screencast](https://youtu.be/9h_61hXCDuI)\n",
    "* [Array API](https://docs.dask.org/en/latest/array-api.html)\n",
    "* [Array examples](https://examples.dask.org/array.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "## What is NumPy?\n",
    "\n",
    "- Python library that provides multidimensional arrays\n",
    "- Includes routines that allow for very fast operations on those arrays\n",
    "- Has a collection of high-level mathematical functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#x = np.ones((20, 20), dtype=int)\n",
    "x = np.ones((10_000, 10_000), dtype=int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a 20,000 X 20,000 matrix with random values and calculate the mean:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z = np.random.random(size=(20_000, 20_000))\n",
    "z.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "    - Create a 10 million x 10 million matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "h = np.ones((10_000_000, 10_000_000), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is where Dask can help us!\n",
    "\n",
    "Dask Array implements a subset of the NumPy ndarray interface using memory optimization techniques (blocked algorithms), breaking large arrays into many smaller ones.\n",
    "\n",
    "In this way, Dask can compute arrays larger than memory while using all available cores.\n",
    "\n",
    "![Dask arrays diagram](images/dask-array-graph.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "x = da.ones((10_000, 10_000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This show the creation of a Dask array using the `dask.array` module. In this example, a `10,000 x 10,000` array of ones is generated, which is then divided into chunks of `1,000 x 1,000`.\n",
    "\n",
    "### Explanation of Elements:\n",
    "\n",
    "- **Array (Bytes):** The total size of the array is approximately 762.94 MiB.\n",
    "- **Shape:** The array has a shape of `(10,000, 10,000)`, with each chunk sized at `(1,000, 1,000)`.\n",
    "- **Dask Graph:** The Dask task graph is composed of 100 chunks arranged in a layered graph. This structure enables parallel computation and efficient memory usage.\n",
    "- **Data Type:** The elements of the array are of type `float64`, which represents 64-bit floating-point numbers, a standard in NumPy.\n",
    "\n",
    "On the right side of the visualization, you can see how Dask partitions the entire array into 100 smaller chunks of `1,000 x 1,000`. This division facilitates efficient computation, optimizing both memory usage and parallel processing capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = x.sum()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is it possible that it took Wall time: 1.56 ms?\n",
    "\n",
    "\n",
    "The reason it took such a short time, Wall time: 1.56 ms, is that the computation hasn't actually been processed yet. Dask only created the computation graph, which outlines the tasks to be executed. The actual computation will only occur when we explicitly call the `compute` function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "h = da.random.random(size=(1_000_000, 1_000_000), chunks=(1000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep.py -d random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We start the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Locked Algorithms (in a nutshell)\n",
    "\n",
    "Let's look side by side at the sum of the elements of an array using a NumPy array and a Dask array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array\n",
    "a_np = np.ones(10)\n",
    "a_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know we can use `sum()` to calculate the sum of the elements in our array, but to show what a block operation would look like, let's do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np_sum = a_np.sum()\n",
    "a_np_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np_sum = a_np[:5].sum() + a_np[5:].sum()\n",
    "a_np_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that each sum in the above calculation is completely independent, so they can be done in parallel.\n",
    "To do this with the Dask array, we need to define our \"slices\", we do this by defining the number of elements we want per block using the `chunks` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da = da.ones(10, chunks=5)\n",
    "a_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!**\n",
    "\n",
    "Note here that to get two blocks, we specify `chunks=5`, in other words we have 5 elements per block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da_sum = a_da.sum()\n",
    "a_da_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task Charts\n",
    "\n",
    "- In general, the code that humans write relies on compilers or interpreters so that computers can understand what we write. **When we move to parallel execution, there is a desire to shift responsibility from compilers to humans**, as they often bring code analysis, optimization, and execution to the code itself. In these cases, we often represent our program structure explicitly as data within the program itself.\n",
    "\n",
    "- In Dask we use task scheduling, **where we divide our program into many medium-sized tasks or units of calculation. We represent these tasks as nodes in a graph with edges between nodes if one task depends on data produced by another.** We call on a task scheduler to run this graph in a way that respects these data dependencies and takes advantage of parallelism where possible, so that multiple independent tasks can run simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the low level Dask graph using cytoscape\n",
    "a_da_sum.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance comparison\n",
    "---------------------------\n",
    "\n",
    "Let's try a more interesting example. We will create a 30_000 x 30_000 matrix with normally distributed values ​​and take the average along one of its axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xn = np.random.normal(10, 0.1, size=(30_000, 30_000))\n",
    "yn = xn.mean(axis=0)\n",
    "yn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask array version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
    "xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xd.nbytes / 1e9  # Gigabytes de la entrada procesada perezosamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yd = xd.mean(axis=0)\n",
    "yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
    "yd = xd.mean(axis=0)\n",
    "yd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions to think about:** (programming them)\n",
    "\n",
    "* What happens if Dask chunks=(10000,10000)?\n",
    "* What happens if Dask chunks=(30,30)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose good chunk sizes\n",
    "This section was inspired by a Dask blog by Genevieve Buckley, you can read it [here] (https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes)\n",
    "\n",
    "A common problem when starting with the Dask array is determining what is a good chunk size. But what is a good size and how do we determine it?\n",
    "\n",
    "\n",
    "### Get to know the pieces\n",
    "\n",
    "We can think of Dask arrays as a large structure composed of chunks of a smaller size, where these fragments are usually a single `numpy` array, and they are all arranged to form a larger Dask array.\n",
    "\n",
    "If you have a Dask array and want more information about the chunks and their size, you can use the `chunksize` and `chunks` attributes to access this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr = da.random.random((1000, 1000, 1000))\n",
    "darr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we created the Dask array, we did not specify the \"chunks\". Dask has defaulted to `chunks='auto'` which adapts to ideal chunk sizes. For more information on how automatic chunking works you can check out this documentation https://docs.dask.org/en/stable/array-chunks.html#automatic-chunking\n",
    "\n",
    "`darr.chunksize` shows the largest chunk size. If you expect your array to have uniform chunk sizes, this is a good summary of chunk size information. But if your array has irregular chunks, `darr.chunks` will show you the explicit sizes of all chunks across all dimensions of your dask array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our example to explore fragmentation a little more. We can re-shard our array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr = darr.rechunk({0: -1, 1: 100, 2: \"auto\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the translation into English:\n",
    "\n",
    "---\n",
    "\n",
    "### Too Small is a Problem\n",
    "\n",
    "If your chunks are too small, the amount of actual work done by each task is very minimal, and the overhead of coordinating all these tasks results in a highly inefficient process.\n",
    "\n",
    "Generally, a Dask scheduler takes about one millisecond to coordinate a single task. This means we want the computation time to be comparatively large, i.e., on the order of seconds.\n",
    "\n",
    "Genevieve Buckley's intuitive analogy:\n",
    "\n",
    "> Imagine we are building a house. It's quite a big job, and if there were only one worker, it would take too long to build. So we have a team of workers and a foreman. The foreman on site is equivalent to the Dask scheduler: their job is to tell the workers what tasks they should do. Suppose we have a large pile of bricks to build a wall, sitting in the corner of the construction site. If the foreman (the Dask scheduler) tells the workers to go fetch one brick at a time and then carry each one to the place where the wall is being built, you can see how this is going to be very slow and inefficient! The workers spend most of their time moving between the wall and the pile of bricks. Much less time is spent doing the actual work of laying bricks in the wall. Instead, we can do this in a smarter way. The foreman (Dask scheduler) can tell the workers to go and bring a wheelbarrow full of bricks each time. Now the workers spend much less time moving between the wall and the pile of bricks, and the wall will be finished much faster.\n",
    "\n",
    "### Too Large is a Problem\n",
    "\n",
    "If your chunks are too large, this is also a problem because you are likely to run out of memory. You will start to see on the dashboard that data is spilling to disk, which will lead to decreased performance.\n",
    "\n",
    "If we load too much data into memory, Dask workers will start spilling data to disk to avoid failures. Spilling data to disk will significantly slow things down due to all the extra read and write operations on the disk. Definitely, this is a situation we want to avoid. To keep an eye on this, you can check the worker memory diagram on the dashboard. Orange bars are a warning that you are close to the limit, and gray bars indicate that data is spilling to disk—this is not good! For more tips, refer to the section on using the Dask dashboard below. For more information on the memory diagram, see the [dashboard documentation](https://docs.dask.org/en/stable/dashboard.html#bytes-stored-and-bytes-per-worker).\n",
    "\n",
    "### Rules of Thumb\n",
    "\n",
    "- Users have reported that chunk sizes smaller than 1 MB tend to be problematic. In general, a chunk size between **100 MB and 1 GB is good**, while going beyond 1 or 2 GB means you have a really large dataset and/or a lot of memory available for the worker.\n",
    "- Upper limit: Avoid very large task graphs. More than 10,000 or 100,000 chunks can start to perform poorly.\n",
    "- Lower limit: To take advantage of parallelization, you need the number of chunks to be at least equal to the number of available worker cores (or better, the number of worker cores multiplied by 2). Otherwise, some workers will remain idle.\n",
    "- The time required to compute each task should be much greater than the time needed to schedule the task. The Dask scheduler takes about 1 millisecond to coordinate a single task, so a good task computation time would be on the order of seconds (not milliseconds).\n",
    "- Chunks should align with the array storage on disk. Modern NDArray storage formats (HDF5, NetCDF, TIFF, Zarr) allow arrays to be stored in chunks so that data blocks can be efficiently retrieved. However, data stores often fragment more finely than ideal for Dask arrays, so it is common to choose a chunking strategy that is a multiple of your storage chunk size, otherwise, you might incur significant overhead. For example, if you are loading data chunked in blocks of (100, 100), you might choose a chunking strategy closer to (1000, 2000), which is larger but still divisible by (100, 100).\n",
    "\n",
    "For more tips on chunking, see https://docs.dask.org/en/stable/array-chunks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cierra tu clúster\n",
    "\n",
    "Es una buena práctica cerrar cualquier clúster de Dash que cree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:data_analysis]",
   "language": "python",
   "name": "conda-env-data_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
